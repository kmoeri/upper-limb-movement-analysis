{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocess movement data from Hands and Pose model",
   "id": "3676db0a991395fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1) Imports",
   "id": "890f7ef0ed2d1be1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "import tqdm\n",
    "\n",
    "# modules\n",
    "from src import data_logging as data_log\n",
    "import motion_processing\n",
    "import render_landmarks"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Initialize Logger and Prepare Movement Data Files",
   "id": "d617a1a9af5ae140"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize logger\n",
    "logger = data_log.setup_batch_logger(log_file_path='batch_preprocessing_logs.txt')\n",
    "\n",
    "# read config data\n",
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "\n",
    "# framerate\n",
    "FRAMERATE: float = config['camera_params']['framerate']\n",
    "\n",
    "# define relevant paths\n",
    "prepro_src_path: str = config['batch_preprocessing']['prepro_src_path']\n",
    "prepro_out_path: str = config['batch_preprocessing']['prepro_out_path']\n",
    "\n",
    "# participant info on affected side\n",
    "affected_side_lst: list[list] = config['participant_info']['affected_side']\n",
    "affected_side_dict: dict = {item[0]: item[1:][0] for item in affected_side_lst}"
   ],
   "id": "437d9c6caea2b1b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3) Batch Preprocess Data",
   "id": "345729a1ec8d38cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# threshold max nan gap (1/5 s)\n",
    "MAX_NAN_GAP: int = int(FRAMERATE) // 5\n",
    "\n",
    "# find all motion files ('.csv')\n",
    "motion_data_file_lst: list[str] = [os.path.join(path, file)\n",
    "                                   for path, dirs, files in os.walk(prepro_src_path)\n",
    "                                   for file in files if file.endswith('.csv') and not file.startswith('.')]\n",
    "\n",
    "motion_data_file_lst = sorted(motion_data_file_lst)\n",
    "\n",
    "# process each motion file\n",
    "print(f'Starting preprocessing of {len(motion_data_file_lst)} movement files...')\n",
    "for motion_data_fpath in tqdm.tqdm(motion_data_file_lst):\n",
    "\n",
    "    # initialize the max consecutive NaN count for each motion data file\n",
    "    max_nan_cnt: int = 0\n",
    "\n",
    "    try:\n",
    "        # get the participant id, affected side ('L' or 'R'), and trial id\n",
    "        participant_id: str = os.path.basename(motion_data_fpath).split('_')[1]\n",
    "        curr_affected_side: str = affected_side_dict[participant_id]\n",
    "        trial_id: int = int(os.path.basename(motion_data_fpath).split('_')[-3][-2:])\n",
    "        model_type: str = (os.path.basename(motion_data_fpath).split('_')[-1]).split('.')[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to parse metadata from {motion_data_fpath}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # processed file name\n",
    "    new_file_name_path: str = motion_data_fpath.split('.')[0] + '_processed.csv'\n",
    "    new_file_out_path: str = os.path.join(prepro_out_path, '/'.join(new_file_name_path.rsplit('/', maxsplit=3)[-3:]))\n",
    "\n",
    "    # prevent overwriting processed files\n",
    "    if os.path.exists(new_file_out_path):\n",
    "        print(f'Skipping {os.path.basename(motion_data_fpath)}: Processed file already exists.')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # read file\n",
    "        motion_df: pd.DataFrame = pd.read_csv(motion_data_fpath)\n",
    "\n",
    "        # extract arms or hands of focus for preprocessing\n",
    "        if model_type == 'hands':\n",
    "            # select hand of focus\n",
    "            side_of_focus_df, side_of_focus_id = motion_processing.extract_hand_of_focus(motion_df, curr_affected_side, trial_id)\n",
    "\n",
    "        elif model_type == 'pose':\n",
    "            # select pose of focus\n",
    "            side_of_focus_df, side_of_focus_id = motion_processing.extract_pose_of_focus(motion_df, curr_affected_side, trial_id)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Unknown model type: {model_type}')\n",
    "\n",
    "        # preprocess data and dump filtered motion\n",
    "        processed_motion_df, nan_cnt = motion_processing.preprocess_motion_data(df=side_of_focus_df, framerate=FRAMERATE)\n",
    "        processed_motion_df.to_csv(new_file_out_path, index=False)\n",
    "\n",
    "        # Check if the dataframe contains any valid numerical data\n",
    "        if processed_motion_df.select_dtypes(include=np.number).empty or processed_motion_df.select_dtypes(include=np.number).dropna(how='all').empty:\n",
    "            raise ValueError('Processed DataFrame contains no numerical data (all NaNs). Skipping plotting image and video.')\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'Processing failed for {motion_data_fpath}. Error: {e}')\n",
    "        nan_cnt = 9001  # indicate error in the warning log\n",
    "        side_of_focus_id = ''\n",
    "\n",
    "\n",
    "    # log INFO line\n",
    "    info_log_data = {\n",
    "        'PID': participant_id,\n",
    "        'AFCT_SIDE': curr_affected_side,\n",
    "        'FCS_SIDE': side_of_focus_id,\n",
    "        'TID': trial_id,\n",
    "        'FPATH': motion_data_fpath,\n",
    "    }\n",
    "    logger.info('File processing finished.', extra=info_log_data)\n",
    "\n",
    "    # log WARNING line\n",
    "    if nan_cnt > MAX_NAN_GAP:\n",
    "        warning_log_data = {\n",
    "            'REP_NAN': nan_cnt,\n",
    "        }\n",
    "        logger.warning('Max consecutive NaNs exceeded threshold.', extra=warning_log_data)\n",
    "\n",
    "print('\\n --------- All preprocessing complete! ---------')"
   ],
   "id": "7adadf40498efb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4) Batch Render Videos with Hand/Pose Overlay",
   "id": "94d613a75a939f54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# find all preprocessed hands files ('.csv')\n",
    "hands_data_file_lst: list[str] = [os.path.join(path, file)\n",
    "                                  for path, dirs, files in os.walk(prepro_out_path)\n",
    "                                  for file in files if file.endswith('hands.csv') and not file.startswith('.')]\n",
    "\n",
    "hands_data_file_lst = sorted(hands_data_file_lst)\n",
    "\n",
    "# find all preprocessed pose files ('.csv')\n",
    "pose_data_file_lst: list[str] = [os.path.join(path, file)\n",
    "                                 for path, dirs, files in os.walk(prepro_out_path)\n",
    "                                 for file in files if file.endswith('pose.csv') and not file.startswith('.')]\n",
    "\n",
    "pose_data_file_lst = sorted(pose_data_file_lst)\n",
    "\n",
    "print('Starting batch rendering...')\n",
    "for idx in tqdm.tqdm(range(len(hands_data_file_lst))):\n",
    "\n",
    "    hands_base_path: str = '_'.join(os.path.basename(hands_data_file_lst[idx]).split('_')[:-2])\n",
    "    pose_base_path: str = '_'.join(os.path.basename(pose_data_file_lst[idx]).split('_')[:-2])\n",
    "\n",
    "    if hands_base_path == pose_base_path:\n",
    "\n",
    "        # define paths\n",
    "        new_video_path: str = hands_base_path + '_focused_overlay.mp4'\n",
    "        new_video_out_path = os.path.join(prepro_out_path, os.path.basename(new_video_path))\n",
    "        vid_basename: str = '_'.join(os.path.basename(hands_data_file_lst[idx]).split('_')[:-2]) + '.mp4'\n",
    "        orig_video_path: str = os.path.join(prepro_src_path, vid_basename)\n",
    "\n",
    "        # read landmark data\n",
    "        hand_df: pd.DataFrame = pd.read_csv(hands_data_file_lst[idx])\n",
    "        pose_df: pd.DataFrame = pd.read_csv(pose_data_file_lst[idx])\n",
    "\n",
    "        # render video overlay\n",
    "        render_landmarks.render_focused_hand_and_arm(orig_video_path, hand_df, pose_df, new_video_out_path)\n",
    "\n",
    "print('\\n --------- All rendering complete! ---------')"
   ],
   "id": "c30d6e2ec931fd52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
